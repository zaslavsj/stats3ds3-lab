---
title: |
  | STATS 3DS3
  | Assignment 5 - Bonus
author: "Jonathan Zaslavsky"
date: "03/31/2021"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(GGally)
library(kableExtra)
library(nnet)
library(MASS) # Boston data
library(e1071)
library(cluster)
library(factoextra) # PCA
library(pgmm) # coffee data
```

## Neural networks (seeds data)

Seeds data description: Measurements of geometrical properties of kernels belonging to three different varieties of wheat. A soft X-ray technique and GRAINS package were used to construct all seven, real-valued attributes.

```{r}
seeds <- read.table(
  "https://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt"
  )
colnames(seeds) <- c("area", 
                     "perimeter", 
                     "compactness", 
                     "length_of_kernel", 
                     "width_of_kernel",
                     "asy_coeff", 
                     "length_of_kernel_groove", 
                     "Class")
# Print dataset and explore correlations
summary(seeds)
cor(dplyr::select(seeds, -Class))


dim(seeds) # Shows dimensions of seeds dataset
knitr::kable(head(seeds)) %>%
  kable_styling(latex_options="scale_down")

x <- seeds %>%
  dplyr::select(-Class) %>%
  scale() # Scale the predictors

# Make a 75% training, 25% test split
set.seed(1)

seeds_train_index <- seeds %>%
  mutate(ind = 1:nrow(seeds)) %>%
  group_by(Class) %>%
  mutate(n = n()) %>%
  sample_frac(size = .75, weight = n) %>%
  ungroup() %>%
  pull(ind)

# Create binary outputs for each class of seed
class_labels <- pull(seeds, Class) %>% 
  class.ind() 
knitr::kable(head(class_labels)) %>%
  kable_styling(latex_options="scale_down")

# Create predictor matrix for training/test set and output for training/test set
seeds_train <- x[seeds_train_index, ]
train_class <- class_labels[seeds_train_index,]
seeds_test <- x[-seeds_train_index, ] 
test_class <- class_labels[-seeds_train_index,]

# Make a neural network with 4 nodes in hidden layer and decay weight of 0
nn_seeds <- nnet(
  x = seeds_train, 
  y = train_class, 
  size = 4, 
  decay = 0, 
  softmax = TRUE,
  maxit=500
  )

# Make prediction on response with neural network
nn_pred <- predict(nn_seeds, seeds_test, 
                   type="class")

tab_seeds <- table(slice(
  seeds, 
  -seeds_train_index) %>% pull(Class), 
  nn_pred)

# Compute test error for the neural network
1-sum(diag(tab_seeds))/sum(tab_seeds)
```

## Neural networks (Boston data - quantitative response)

Boston data description: A data from with the housing values in 506 suburbs of Boston based on 14 variables.

```{r}
# Create training/testing sets
train_Boston <- sample(
  1:nrow(Boston), 
  nrow(Boston)/2
  )

# Scale the predictors and response
x <- scale(Boston)

# Create predictor matrix for training/test set and output for training/test set
Boston_train <- x[train_Boston, ]
train_medv <- x[train_Boston, "medv"]
Boston_test <- x[-train_Boston, ] 
test_medv <- x[-train_Boston, "medv"]

# Make a neural network with 10 nodes in hidden layer and decay weight of 1
nn_Boston <- nnet(
  Boston_train, 
  train_medv,  
  size=10, 
  decay=1, 
  softmax=FALSE, 
  maxit=1000,
  linout=TRUE
  )

# Make prediction on house salary response with neural network
nn_pred <- predict(
  nn_Boston, 
  Boston_test,
  type="raw"
  )

plot(test_medv, nn_pred)

# Calculate MSE for the neural network
mean((test_medv - nn_pred)^2)
```

## Neural networks (iris data)

Iris data description: Measurements (cm) of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris: _Iris setosa_, _versicolor_, and _virginica_.

```{r}
## Cross-validation for iris data
set.seed(1)

data("iris")

Species <- pull(iris, Species)

xy <- dplyr::select(iris, -Species) %>%
  scale() %>% 
  data.frame() %>% 
  mutate(Species = Species) # Scale predictors

# Make a 80% training, 20% testing split
iris_train_index <- iris %>%
  mutate(ind = 1:nrow(iris)) %>%
  group_by(Species) %>%
  mutate(n = n()) %>%
  sample_frac(size = .8, weight = n) %>%
  ungroup() %>%
  pull(ind)

iris_train <- slice(xy, iris_train_index)
iris_test <- slice(xy, -iris_train_index)
class_labels <- pull(xy, Species) %>% 
  class.ind() 

# Apply 5-fold CV
iris_nnet1 <- tune.nnet(
  Species~., 
  data = iris_train, 
  size = 1:30, 
  tunecontrol = tune.control(sampling = "cross",cross=5)
  )

head(summary(iris_nnet1))

plot(iris_nnet1)

# Fit the model with the size parameter selected from the CV
nn_iris <- nnet(
  x = dplyr::select(iris_train, -Species),
  y = class_labels[iris_train_index, ],
  size = iris_nnet1$best.parameters[1,1], 
  decay = 0, 
  softmax = TRUE
  )

# Fit the model with the size parameter selected from the CV
nn_iris <- nnet(
  x = dplyr::select(iris_train, -Species),
  y = class_labels[iris_train_index, ],
  size = iris_nnet1$best.parameters[1,1], 
  decay = 0, 
  softmax = TRUE
  )

# Make prediction on response with neural network
nn_pred <- predict(
  nn_iris, 
  dplyr::select(iris_test, -Species), 
  type="class"
  )

tab <- table(pull(iris_test, Species), 
  nn_pred
  )
tab

# Compute test error for the neural network
1- sum(diag(tab))/sum(tab)

## Tune decay and size parameters
set.seed(1)

iris_nnet2 <- tune.nnet(
  Species~., 
  data = iris_train, 
  size = 1:20,
  decay = 0:3,
  tunecontrol = tune.control(sampling = "cross",cross=5)
  )

head(summary(iris_nnet2))

plot(iris_nnet2)

# Fit model with selected size and decay parameters
nn_iris_d_s <- nnet(
  x = dplyr::select(iris_train, -Species),
  y = class_labels[iris_train_index, ], 
  size = iris_nnet2$best.parameters[1,1], 
  decay = iris_nnet2$best.parameters[1,2], 
  softmax = TRUE
  )

# Make prediction on response with neural network
nn_pred <- predict(
  nn_iris_d_s, 
  dplyr::select(iris_test, -Species), 
  type="class"
  )

tab <- table(pull(iris_test, Species), 
  nn_pred
  )
tab

# Compute test error for the neural network
1- sum(diag(tab))/sum(tab)
```

## Clustering (coffee data)

Coffee data description: Data on the chemical composition of coffee samples collected from around the world, comprising 43 samples from 29 countries.

```{r}
data("coffee")
set.seed(1)

# Exclude first two columns of the dataset
x <- dplyr::select(coffee, - Variety, - Country) 
x_scaled <- scale(x)
kmeans_coffee <- kmeans(x_scaled, 2)
kmeans_coffee$tot.withinss
kmeans_coffee <- kmeans(x_scaled, 3)
kmeans_coffee$tot.withinss

# Select K using elbow method
withiclusterss <- function(K,x){
  kmeans(x, K)$tot.withinss
}

K <- 1:8

wcss <- lapply(as.list(K), function(k){
  withiclusterss(k, x_scaled)
}) %>% unlist()

ggplot(tibble(K = K, wcss = wcss), aes(x = K, y = wcss)) +
  geom_point() +
  geom_line() +
  xlab("Number of clusters (k)") +
  ylab("Total within-clusters sum of squares") +
  scale_x_continuous(breaks=c(seq(1,K[length(K)])))

# Elbow method suggest k = 2 is optimal
# Reduce dimensions and plot clusters in 2D
kmeans_coffee <- kmeans(x_scaled, 2)
fvPCA <- fviz_cluster(kmeans_coffee, 
                    x_scaled, 
                    ellipse.type = "norm",
                    main = "Plot the results of k-means clustering after PCA")
fvPCA

# Construct silhouette plots to choose best number of clusters
si <- silhouette(kmeans_coffee$cluster, dist(x_scaled))
head(si)
# Average silhouette width
mean(si[, 3])
plot(si, nmax= 80, cex.names=0.6, main = "")

# Select K using average Silhouette width
avgSilhouette <- function(K,x) {
  km_cl <- kmeans(x, K)
  sil <- silhouette(km_cl$cluster, dist(x)) 
  return(mean(sil[, 3]))
}

K <- 2:8

avgSil <- numeric()
for(i in K){
  avgSil[(i-1)] <- avgSilhouette(i, x_scaled)
}

ggplot(tibble(K = K, avgSil = avgSil), aes(x = K, y = avgSil)) +
  geom_point() +
  geom_line() +
  xlab("Number of clusters (k)") +
  ylab("Average silhouette width") +
  scale_x_continuous(breaks=c(seq(1,K[length(K)])))

# Use k = 2 based on average silhouette width
# Apply k-medoids clustering
kmedoid_coffee <- pam(x_scaled, 2)
kmedoid_coffee$silinfo$avg.width

avgSil <- lapply(as.list(2:8), function(k){
  kmedoid_coffee <- pam(x_scaled, k)
kmedoid_coffee$silinfo$avg.width
}) %>% unlist()

ggplot(tibble(K = 2:8, avgSil = avgSil), aes(x = K, y = avgSil)) +
  geom_point() +
  geom_line() +
  xlab("Number of clusters (k)") +
  ylab("Average silhouette width for k-medoid") +
  scale_x_continuous(breaks=c(seq(1,K[length(K)])))
```

## Clustering (votes data)

Votes data description: A data frame with the percents of votes given to the republican candidate in presidential elections from 1856 to 1976. Rows represent the 50 states, and columns the 31 elections.

```{r}
data(votes.repub) # From cluster package
votes.repub_scaled <- scale(votes.repub)
#votes.repub_kmeans <- kmeans(votes.repub_scaled, 2)

## Apply hierarchical divisive clustering
divisive_votes <- diana(
  votes.repub, 
  metric = "euclidean", 
  stand = TRUE
  )

plot(divisive_votes)

cut_divisive_votes <- cutree(as.hclust(divisive_votes), k = 2)
table(cut_divisive_votes) # 8 and 42 group members
rownames(votes.repub)[cut_divisive_votes == 1]
# rownames(votes.repub)[cut_divisive_votes == 2]

# Make a nice dendrogram
fviz_dend(
  divisive_votes, 
  cex = 0.5,
  k = 2, # Cut in 2 groups
  palette = "jco", # Color palette
  main = "Dendrogram for votes data (divisive clustering)")

## Apply hierarchical agglomerative clustering
x <- votes.repub %>% 
  scale()
hc_vote <- hclust(dist(x), "ward.D")
plot(hc_vote)

# Make a nice dendrogram
fviz_dend(
  hc_vote, 
  k = 2, # Cut in 2 groups
  cex = 0.5, 
  color_labels_by_k = TRUE, 
  rect = TRUE,
  main = "Dendrogram for votes data (agglomerative clustering)"
  )
```
